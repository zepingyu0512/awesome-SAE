# Awesome Papers for Sparse Auto-Encoder (SAE)
This list focuses on sparse auto-encoder (SAE) techniques in mechanistic interpretability. [Another list](https://github.com/zepingyu0512/awesome-llm-understanding-mechanism.git) focuses on understanding the internal mechanism of LLMs.

Paper/preprint/blog recommendation: please release a issue or contact [me](https://zepingyu0512.github.io/).


## Papers

### 2025

- [Probing the Vulnerability of Large Language Models to Polysemantic Interventions](https://arxiv.org/pdf/2505.11611)
   - \[arxiv\] \[2025.5\]

- [SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](https://arxiv.org/pdf/2505.16188)
   - \[arxiv\] \[2025.5\]

- [Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders](https://arxiv.org/pdf/2505.08080)
   - \[arxiv\] \[2025.5\]

- [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)
   - \[Anthropic\] \[2025.3\]

- [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/pdf/2503.05613)
   - \[SAE survey\] \[2025.3\]

- [SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability](https://arxiv.org/pdf/2503.09532)
   - \[arxiv\] \[2025.3\]

- [Sparse Autoencoders Do Not Find Canonical Units of Analysis](https://www.arxiv.org/pdf/2502.04878)
   - \[ICLR 2025\] \[2025.2\]

- [Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders](https://arxiv.org/pdf/2502.15576)
   - \[arxiv\] \[2025.2\]

- [Scaling Sparse Feature Circuits For Studying In-Context Learning](https://openreview.net/pdf?id=Pa1vr1Prww)
   - \[openreview\] \[2025.1\]

- [AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders](https://arxiv.org/pdf/2501.17148)
   - \[ICML 2025\] \[2025.1\]

- [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/pdf/2501.08319)
   - \[arxiv\] \[2025.1\]

### 2024

- [BatchTopK Sparse Autoencoders](https://arxiv.org/pdf/2412.06410)
   - \[arxiv\] \[2024.12\]

- [Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models](https://arxiv.org/pdf/2411.14257)
   - \[ICLR 2025\] \[2024.11\]

- [Improving Steering Vectors by Targeting Sparse Autoencoder Features](https://arxiv.org/pdf/2411.02193?)
   - \[arxiv\] \[2024.11\]

- [Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks](https://arxiv.org/pdf/2411.18895)
   - \[arxiv\] \[2024.11\]

- [Applying sparse autoencoders to unlearn knowledge in language models](https://arxiv.org/pdf/2410.19278)
   - \[arxiv\] \[2024.10\]

- [Evaluating feature steering: A case study in mitigating social biases](https://www.anthropic.com/research/evaluating-feature-steering)
   - \[Anthropic\] \[2024.10\]

- [Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders](https://arxiv.org/pdf/2410.20526)
   - \[arxiv\] \[2024.10\]
 
- [Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups](https://arxiv.org/pdf/2410.21508)
   - \[arxiv\] \[2024.10\]

- [Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models](https://arxiv.org/pdf/2410.01280)
   - \[ICLR 2025\] \[2024.10\]

- [Scaling Automatic Neuron Description](https://transluce.org/neuron-descriptions)
   - \[transluce\] \[2024.10\]

- [Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small](https://arxiv.org/pdf/2409.04478)
   - \[arxiv\] \[2024.9\]

- [Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://arxiv.org/pdf/2408.05147)
   - \[Deepmind\] \[2024.8\]
 
- [Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders](https://arxiv.org/pdf/2407.14435)
   - \[Deepmind\] \[2024.8\]

- [Self-explaining SAE features](https://www.lesswrong.com/posts/8ev6coxChSWcxCDy8/self-explaining-sae-features)
   - \[Lesswrong blog\] \[2024.8\]

- [SAEs (usually) Transfer Between Base and Chat Models](https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models)
   - \[AI alignment forum blog\] \[2024.7\]

- [BatchTopK: A Simple Improvement for TopK-SAEs](https://www.lesswrong.com/posts/Nkx6yWZNbAsfvic98/batchtopk-a-simple-improvement-for-topk-saes)
   - \[Lesswrong blog\] \[2024.7\]

- [Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models](https://arxiv.org/pdf/2408.00113)
   - \[ICML MI workshop\] \[2024.7\]
 
- [Interpreting Attention Layer Outputs with Sparse Autoencoders](https://arxiv.org/pdf/2406.17759)
   - \[arxiv\] \[2024.6\]

- [Scaling and evaluating sparse autoencoders](https://arxiv.org/pdf/2406.04093)
   - \[OpenAI\] \[2024.6\]

- [Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning](https://arxiv.org/pdf/2405.12241)
   - \[NeurIPS 2024\] \[2024.5\]

- [Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control](https://arxiv.org/pdf/2405.08366)
   - \[arxiv\] \[2024.5\]

- [Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization](https://arxiv.org/pdf/2406.00045)
   - \[arxiv\] \[2024.5\]

- [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
   - \[Anthropic\] \[2024.5\]

- [How to use and interpret activation patching](https://arxiv.org/pdf/2404.15255)
   - \[arxiv\] \[2024.4\]

- [Improving Dictionary Learning with Gated Sparse Autoencoders](https://arxiv.org/pdf/2404.16014)
   - \[Deepmind\] \[2024.4\]

- [Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](https://arxiv.org/pdf/2403.19647v1)
   - \[arxiv\] \[2024.3\]

- [RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations](https://arxiv.org/pdf/2402.17700)
   - \[ACL 2024\] \[2024.2\]

- [Addressing Feature Suppression in SAEs](https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes)
   - \[lesswrong blog\] \[2024.2\]

- [Examining Language Model Performance with Reconstructed Activations using Sparse Autoencoders](https://www.lesswrong.com/posts/8QRH8wKcnKGhpAu2o/examining-language-model-performance-with-reconstructed)
   - \[lesswrong blog\] \[2024.2\]

- [Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT](https://arxiv.org/pdf/2402.12201)
   - \[arxiv\] \[2024.2\]

- [Open Source Sparse Autoencoders for all Residual Stream Layers of GPT2-Small](https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream)
   - \[alignmentforum blog\] \[2024.2\]
 
### 2023

- [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/pdf/2312.06681)
   - \[ACL 2024\] \[2023.12\]
 
- [Codebook Features: Sparse and Discrete Interpretability for Neural Networks](https://arxiv.org/pdf/2310.17230)
   - \[arxiv\] \[2023.10\]

- [Attribution patching outperforms automated circuit discovery](https://arxiv.org/pdf/2310.10348)
   - \[BlackboxNLP 2024\] \[2023.10\]

- [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)
   - \[Anthropic\] \[2023.10\]
 
- [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://arxiv.org/pdf/2309.08600)
   - \[ICLR 2024\] \[2023.9\]

- [Steering Language Models With Activation Engineering](https://arxiv.org/pdf/2308.10248)
   - \[arxiv\] \[2023.8\]

- [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model](https://arxiv.org/pdf/2306.03341)
   - \[NeurIPS 2023\] \[2023.6\]

- [Language models can explain neurons in language models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)
   - \[OpenAI\] \[2023.5\]

- [Distributed Representations: Composition & Superposition](https://transformer-circuits.pub/2023/superposition-composition/index.html)
   - \[Anthropic\] \[2023.5\]

- [Privileged Bases in the Transformer Residual Stream](https://transformer-circuits.pub/2023/privileged-basis/index.html)
   - \[Anthropic\] \[2023.3\]

- [Attribution Patching: Activation Patching At Industrial Scale](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)
   - \[Neel Nanda Blog\] \[2023.2\]
 
### 2022

- [Causal Scrubbing: a method for rigorously testing interpretability hypotheses](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing)
   - \[AI alignment forum\] \[2022.12\]

- [Taking features out of superposition with sparse autoencoders](https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition)
   - \[AI alignment forum\] \[2022.12\]

- [Engineering Monosemanticity in Toy Models](https://arxiv.org/pdf/2211.09169)
   - \[arxiv\] \[2022.11\]

- [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892)
   - \[arxiv\] \[2022.9\]

- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
   - \[Anthropic\] \[2022.9\]
