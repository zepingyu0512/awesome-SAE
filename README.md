# Awesome Papers for Sparse Auto-Encoder (SAE)
This list focuses on sparse auto-encoder (SAE) techniques in mechanistic interpretability. [Another repo](https://github.com/zepingyu0512/awesome-llm-understanding-mechanism.git) focuses on understanding the internal mechanism of LLMs.

Conference paper recommendation: please release a issue or contact [me](https://zepingyu0512.github.io/).


## Papers

- [Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small](https://arxiv.org/pdf/2409.04478)
   - \[arxiv\] \[2024.9\]

- [Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://arxiv.org/pdf/2408.05147)
   - \[Deepmind\] \[2024.8\]
 
- [Interpreting Attention Layer Outputs with Sparse Autoencoders](https://arxiv.org/pdf/2406.17759)
   - \[arxiv\] \[2024.6\]

- [Scaling and evaluating sparse autoencoders](https://arxiv.org/pdf/2406.04093)
   - \[OpenAI\] \[2024.6\]

- [Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning](https://arxiv.org/pdf/2405.12241)
   - \[NeurIPS 2024\] \[2024.5\]

- [Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control](https://arxiv.org/pdf/2405.08366)
   - \[arxiv\] \[2024.5\]

- [Improving Dictionary Learning with Gated Sparse Autoencoders](https://arxiv.org/pdf/2404.16014)
   - \[arxiv\] \[2024.5\]

- [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
   - \[Anthropic\] \[2024.5\]

- [RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations](https://arxiv.org/pdf/2402.17700)
   - \[ACL 2024\] \[2024.2\]

- [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)
   - \[Anthropic\] \[2023.10\]
 
- [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://arxiv.org/pdf/2309.08600)
   - \[ICLR 2024\] \[2023.9\]

- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
   - \[Anthropic\] \[2022.9\]



