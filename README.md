# Awesome Papers for Sparse Auto-Encoder (SAE)
This list focuses on sparse auto-encoder (SAE) techniques in mechanistic interpretability. [Another list](https://github.com/zepingyu0512/awesome-llm-understanding-mechanism.git) focuses on understanding the internal mechanism of LLMs.

Paper/blog recommendation (arxiv/lesswrong is OK): please release a issue or contact [me](https://zepingyu0512.github.io/).


## Papers

- [Improving Steering Vectors by Targeting Sparse Autoencoder Features](https://arxiv.org/pdf/2411.02193?)
   - \[arxiv\] \[2024.11\]

- [Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks](https://arxiv.org/pdf/2411.18895)
   - \[arxiv\] \[2024.11\]

- [Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small](https://arxiv.org/pdf/2409.04478)
   - \[arxiv\] \[2024.9\]

- [Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://arxiv.org/pdf/2408.05147)
   - \[Deepmind\] \[2024.8\]
 
- [Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders](https://arxiv.org/pdf/2407.14435)
   - \[Deepmind\] \[2024.8\]

- [SAEs (usually) Transfer Between Base and Chat Models](https://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models)
   - \[AI alignment forum blog\] \[2024.7\]
 
- [Interpreting Attention Layer Outputs with Sparse Autoencoders](https://arxiv.org/pdf/2406.17759)
   - \[arxiv\] \[2024.6\]

- [Scaling and evaluating sparse autoencoders](https://arxiv.org/pdf/2406.04093)
   - \[OpenAI\] \[2024.6\]

- [Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning](https://arxiv.org/pdf/2405.12241)
   - \[NeurIPS 2024\] \[2024.5\]

- [Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control](https://arxiv.org/pdf/2405.08366)
   - \[arxiv\] \[2024.5\]

- [Improving Dictionary Learning with Gated Sparse Autoencoders](https://arxiv.org/pdf/2404.16014)
   - \[arxiv\] \[2024.5\]

- [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
   - \[Anthropic\] \[2024.5\]

- [How to use and interpret activation patching](https://arxiv.org/pdf/2404.15255)
   - \[Anthropic\] \[2024.4\]

- [Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](https://arxiv.org/pdf/2403.19647v1)
   - \[arxiv\] \[2024.3\]

- [RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations](https://arxiv.org/pdf/2402.17700)
   - \[ACL 2024\] \[2024.2\]

- [Addressing Feature Suppression in SAEs](https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes)
   - \[lesswrong blog\] \[2024.2\]

- [Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT](https://arxiv.org/pdf/2402.12201)
   - \[arxiv\] \[2024.2\]

- [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/pdf/2312.06681)
   - \[ACL 2024\] \[2023.12\]
 
- [Codebook Features: Sparse and Discrete Interpretability for Neural Networks](https://arxiv.org/pdf/2310.17230)
   - \[arxiv\] \[2023.10\]

- [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)
   - \[Anthropic\] \[2023.10\]
 
- [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://arxiv.org/pdf/2309.08600)
   - \[ICLR 2024\] \[2023.9\]

- [Steering Language Models With Activation Engineering](https://arxiv.org/pdf/2308.10248)
   - \[arxiv\] \[2023.8\]

- [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model](https://arxiv.org/pdf/2306.03341)
   - \[NeurIPS 2023\] \[2023.6\]

- [Language models can explain neurons in language models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)
   - \[OpenAI\] \[2023.5\]

- [Distributed Representations: Composition & Superposition](https://transformer-circuits.pub/2023/superposition-composition/index.html)
   - \[Anthropic\] \[2023.5\]

- [Privileged Bases in the Transformer Residual Stream](https://transformer-circuits.pub/2023/privileged-basis/index.html)
   - \[Anthropic\] \[2023.3\]

- [Attribution Patching: Activation Patching At Industrial Scale](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)
   - \[Neel Nanda Blog\] \[2023.2\]

- [Engineering Monosemanticity in Toy Models](https://arxiv.org/pdf/2211.09169)
   - \[arxiv\] \[2022.11\]

- [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892)
   - \[arxiv\] \[2022.9\]

- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
   - \[Anthropic\] \[2022.9\]




